# This workflow will build and push a new container image to Amazon ECR,
# and then will deploy a new task definition to Amazon ECS, when there is a push to the "main" branch.
#
# To use this workflow, you will need to complete the following set-up steps:
#
# 1. Create an ECR repository to store your images.
#    For example: `aws ecr create-repository --repository-name my-ecr-repo --region us-east-2`.
#    Replace the value of the `ECR_REPOSITORY` environment variable in the workflow below with your repository's name.
#    Replace the value of the `AWS_REGION` environment variable in the workflow below with your repository's region.
#
# 2. Create an ECS task definition, an ECS cluster, and an ECS service.
#    For example, follow the Getting Started guide on the ECS console:
#      https://us-east-2.console.aws.amazon.com/ecs/home?region=us-east-2#/firstRun
#    Replace the value of the `ECS_SERVICE` environment variable in the workflow below with the name you set for the Amazon ECS service.
#    Replace the value of the `ECS_CLUSTER` environment variable in the workflow below with the name you set for the cluster.
#
# 3. Store your ECS task definition as a JSON file in your repository.
#    The format should follow the output of `aws ecs register-task-definition --generate-cli-skeleton`.
#    Replace the value of the `ECS_TASK_DEFINITION` environment variable in the workflow below with the path to the JSON file.
#    Replace the value of the `CONTAINER_NAME` environment variable in the workflow below with the name of the container
#    in the `containerDefinitions` section of the task definition.
#
# 4. Store an IAM user access key in GitHub Actions secrets named `AWS_ACCESS_KEY_ID` and `AWS_SECRET_ACCESS_KEY`.
#    See the documentation for each action used below for the recommended IAM policies for this IAM user,
#    and best practices on handling the access key credentials.

name: Deploy and Run scrap.py on EC2

on:
  push:
    branches: [ "main" ]

jobs:
  deploy:
    runs-on: ubuntu-latest
    environment: production

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up SSH
        run: |
          echo "${{ secrets.PEM_KEY }}" > github-deploy-key.pem
          chmod 600 github-deploy-key.pem
          mkdir -p ~/.ssh
          ssh-keyscan -H ${{ secrets.EC2_PUBLIC_IP }} >> ~/.ssh/known_hosts

      - name: Copy files to EC2
        run: |
          scp -i github-deploy-key.pem scrap.py ubuntu@${{ secrets.EC2_PUBLIC_IP }}:/home/ubuntu/scrap.py
          scp -i github-deploy-key.pem service_providers.json ubuntu@${{ secrets.EC2_PUBLIC_IP }}:/home/ubuntu/service_providers.json

      - name: Install dependencies on EC2
        run: |
          ssh -i github-deploy-key.pem ubuntu@${{ secrets.EC2_PUBLIC_IP }} << 'EOF'
            sudo apt update
            sudo apt install -y python3-pip chromium-chromedriver
            pip3 install selenium webdriver-manager
          EOF

      - name: Run script on EC2
        run: |
          ssh -i github-deploy-key.pem ubuntu@${{ secrets.EC2_PUBLIC_IP }} 'python3 /home/ubuntu/scrap.py'

      - name: Retrieve output file from EC2
        run: |
          scp -i github-deploy-key.pem ubuntu@${{ secrets.EC2_PUBLIC_IP }}:/home/ubuntu/service_providers_details.csv .

      - name: Upload output file as an artifact
        uses: actions/upload-artifact@v2
        with:
          name: output
          path: service_providers_details.csv
